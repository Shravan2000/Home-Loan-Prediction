{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan_ID               0\n",
      "Gender               13\n",
      "Married               3\n",
      "Dependents           15\n",
      "Education             0\n",
      "Self_Employed        32\n",
      "ApplicantIncome       0\n",
      "CoapplicantIncome     0\n",
      "LoanAmount           22\n",
      "Loan_Amount_Term     14\n",
      "Credit_History       50\n",
      "Property_Area         0\n",
      "Loan_Status           0\n",
      "dtype: int64\n",
      "      Loan_ID  Gender Married Dependents     Education Self_Employed  \\\n",
      "0    LP001002    Male      No          0      Graduate            No   \n",
      "1    LP001003    Male     Yes          1      Graduate            No   \n",
      "2    LP001005    Male     Yes          0      Graduate           Yes   \n",
      "3    LP001006    Male     Yes          0  Not Graduate            No   \n",
      "4    LP001008    Male      No          0      Graduate            No   \n",
      "..        ...     ...     ...        ...           ...           ...   \n",
      "609  LP002978  Female      No          0      Graduate            No   \n",
      "610  LP002979    Male     Yes         3+      Graduate            No   \n",
      "611  LP002983    Male     Yes          1      Graduate            No   \n",
      "612  LP002984    Male     Yes          2      Graduate            No   \n",
      "613  LP002990  Female      No          0      Graduate           Yes   \n",
      "\n",
      "     ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
      "0               5849                0.0         NaN             360.0   \n",
      "1               4583             1508.0       128.0             360.0   \n",
      "2               3000                0.0        66.0             360.0   \n",
      "3               2583             2358.0       120.0             360.0   \n",
      "4               6000                0.0       141.0             360.0   \n",
      "..               ...                ...         ...               ...   \n",
      "609             2900                0.0        71.0             360.0   \n",
      "610             4106                0.0        40.0             180.0   \n",
      "611             8072              240.0       253.0             360.0   \n",
      "612             7583                0.0       187.0             360.0   \n",
      "613             4583                0.0       133.0             360.0   \n",
      "\n",
      "     Credit_History Property_Area Loan_Status  \n",
      "0               1.0         Urban           Y  \n",
      "1               1.0         Rural           N  \n",
      "2               1.0         Urban           Y  \n",
      "3               1.0         Urban           Y  \n",
      "4               1.0         Urban           Y  \n",
      "..              ...           ...         ...  \n",
      "609             1.0         Rural           Y  \n",
      "610             1.0         Rural           Y  \n",
      "611             1.0         Urban           Y  \n",
      "612             1.0         Urban           Y  \n",
      "613             0.0     Semiurban           N  \n",
      "\n",
      "[614 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('loan_train.csv')\n",
    "print(df.isnull().sum())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10953093.85135135\n"
     ]
    }
   ],
   "source": [
    "features=['Married','Education','Self_Employed','ApplicantIncome','CoapplicantIncome','LoanAmount','Loan_Amount_Term','Credit_History','Property_Area']\n",
    "df['LoanAmount']=df['LoanAmount']*1000*74.81\n",
    "df['ApplicantIncome']=df['ApplicantIncome']*74.81\n",
    "df['CoapplicantIncome']=df['CoapplicantIncome']*74.81\n",
    "X=df[features]\n",
    "Y=df['Loan_Status']\n",
    "print(df['LoanAmount'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
      "0          437563.69               0.00         NaN             360.0   \n",
      "1          342854.23          112813.48   9575680.0             360.0   \n",
      "2          224430.00               0.00   4937460.0             360.0   \n",
      "3          193234.23          176401.98   8977200.0             360.0   \n",
      "4          448860.00               0.00  10548210.0             360.0   \n",
      "..               ...                ...         ...               ...   \n",
      "609        216949.00               0.00   5311510.0             360.0   \n",
      "610        307169.86               0.00   2992400.0             180.0   \n",
      "611        603866.32           17954.40  18926930.0             360.0   \n",
      "612        567284.23               0.00  13989470.0             360.0   \n",
      "613        342854.23               0.00   9949730.0             360.0   \n",
      "\n",
      "     Credit_History  Married_Yes  Education_Not Graduate  Self_Employed_Yes  \\\n",
      "0               1.0            0                       0                  0   \n",
      "1               1.0            1                       0                  0   \n",
      "2               1.0            1                       0                  1   \n",
      "3               1.0            1                       1                  0   \n",
      "4               1.0            0                       0                  0   \n",
      "..              ...          ...                     ...                ...   \n",
      "609             1.0            0                       0                  0   \n",
      "610             1.0            1                       0                  0   \n",
      "611             1.0            1                       0                  0   \n",
      "612             1.0            1                       0                  0   \n",
      "613             0.0            0                       0                  1   \n",
      "\n",
      "     Property_Area_Semiurban  Property_Area_Urban  \n",
      "0                          0                    1  \n",
      "1                          0                    0  \n",
      "2                          0                    1  \n",
      "3                          0                    1  \n",
      "4                          0                    1  \n",
      "..                       ...                  ...  \n",
      "609                        0                    0  \n",
      "610                        0                    0  \n",
      "611                        0                    1  \n",
      "612                        0                    1  \n",
      "613                        1                    0  \n",
      "\n",
      "[614 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "X=pd.get_dummies(X,drop_first=True)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['Married_Yes']=X['Married_Yes'].fillna(value=0)\n",
    "X['Self_Employed_Yes']=X['Self_Employed_Yes'].fillna(value=0)\n",
    "X['LoanAmount']=X['LoanAmount'].fillna(value=df['LoanAmount'].value_counts().idxmax())\n",
    "X['Loan_Amount_Term']=X['Loan_Amount_Term'].fillna(value=X['Loan_Amount_Term'].value_counts().idxmax())\n",
    "X['Credit_History']=X['Credit_History'].fillna(value=X['Credit_History'].value_counts().idxmax())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
      "0          437563.69               0.00   8977200.0             360.0   \n",
      "1          342854.23          112813.48   9575680.0             360.0   \n",
      "2          224430.00               0.00   4937460.0             360.0   \n",
      "3          193234.23          176401.98   8977200.0             360.0   \n",
      "4          448860.00               0.00  10548210.0             360.0   \n",
      "..               ...                ...         ...               ...   \n",
      "609        216949.00               0.00   5311510.0             360.0   \n",
      "610        307169.86               0.00   2992400.0             180.0   \n",
      "611        603866.32           17954.40  18926930.0             360.0   \n",
      "612        567284.23               0.00  13989470.0             360.0   \n",
      "613        342854.23               0.00   9949730.0             360.0   \n",
      "\n",
      "     Credit_History  Married_Yes  Education_Not Graduate  Self_Employed_Yes  \\\n",
      "0               1.0            0                       0                  0   \n",
      "1               1.0            1                       0                  0   \n",
      "2               1.0            1                       0                  1   \n",
      "3               1.0            1                       1                  0   \n",
      "4               1.0            0                       0                  0   \n",
      "..              ...          ...                     ...                ...   \n",
      "609             1.0            0                       0                  0   \n",
      "610             1.0            1                       0                  0   \n",
      "611             1.0            1                       0                  0   \n",
      "612             1.0            1                       0                  0   \n",
      "613             0.0            0                       0                  1   \n",
      "\n",
      "     Property_Area_Semiurban  Property_Area_Urban  \n",
      "0                          0                    1  \n",
      "1                          0                    0  \n",
      "2                          0                    1  \n",
      "3                          0                    1  \n",
      "4                          0                    1  \n",
      "..                       ...                  ...  \n",
      "609                        0                    0  \n",
      "610                        0                    0  \n",
      "611                        0                    1  \n",
      "612                        0                    1  \n",
      "613                        1                    0  \n",
      "\n",
      "[614 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv('df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      1.0\n",
      "1      1.0\n",
      "2      1.0\n",
      "3      1.0\n",
      "4      1.0\n",
      "      ... \n",
      "609    1.0\n",
      "610    1.0\n",
      "611    1.0\n",
      "612    1.0\n",
      "613    0.0\n",
      "Name: Credit_History, Length: 614, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(X['Credit_History'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
      "0          437563.69               0.00   8977200.0             360.0   \n",
      "1          342854.23          112813.48   9575680.0             360.0   \n",
      "2          224430.00               0.00   4937460.0             360.0   \n",
      "3          193234.23          176401.98   8977200.0             360.0   \n",
      "4          448860.00               0.00  10548210.0             360.0   \n",
      "..               ...                ...         ...               ...   \n",
      "609        216949.00               0.00   5311510.0             360.0   \n",
      "610        307169.86               0.00   2992400.0             180.0   \n",
      "611        603866.32           17954.40  18926930.0             360.0   \n",
      "612        567284.23               0.00  13989470.0             360.0   \n",
      "613        342854.23               0.00   9949730.0             360.0   \n",
      "\n",
      "     Credit_History  Married_Yes  Education_Not Graduate  Self_Employed_Yes  \\\n",
      "0               1.0            0                       0                  0   \n",
      "1               1.0            1                       0                  0   \n",
      "2               1.0            1                       0                  1   \n",
      "3               1.0            1                       1                  0   \n",
      "4               1.0            0                       0                  0   \n",
      "..              ...          ...                     ...                ...   \n",
      "609             1.0            0                       0                  0   \n",
      "610             1.0            1                       0                  0   \n",
      "611             1.0            1                       0                  0   \n",
      "612             1.0            1                       0                  0   \n",
      "613             0.0            0                       0                  1   \n",
      "\n",
      "     Property_Area_Semiurban  Property_Area_Urban  \n",
      "0                          0                    1  \n",
      "1                          0                    0  \n",
      "2                          0                    1  \n",
      "3                          0                    1  \n",
      "4                          0                    1  \n",
      "..                       ...                  ...  \n",
      "609                        0                    0  \n",
      "610                        0                    0  \n",
      "611                        0                    1  \n",
      "612                        0                    1  \n",
      "613                        1                    0  \n",
      "\n",
      "[614 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ApplicantIncome            0\n",
      "CoapplicantIncome          0\n",
      "LoanAmount                 0\n",
      "Loan_Amount_Term           0\n",
      "Credit_History             0\n",
      "Married_Yes                0\n",
      "Education_Not Graduate     0\n",
      "Self_Employed_Yes          0\n",
      "Property_Area_Semiurban    0\n",
      "Property_Area_Urban        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc=MinMaxScaler(feature_range = (0,1)).fit(X.values)\n",
    "X_train=sc.transform(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07048856 0.         0.16063676 ... 0.         0.         1.        ]\n",
      " [0.05482993 0.03619171 0.17221418 ... 0.         0.         0.        ]\n",
      " [0.03525046 0.         0.08248915 ... 1.         0.         1.        ]\n",
      " ...\n",
      " [0.09798392 0.00575995 0.35311143 ... 0.         0.         1.        ]\n",
      " [0.09193568 0.         0.25759768 ... 0.         0.         1.        ]\n",
      " [0.05482993 0.         0.17945007 ... 1.         1.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "values={'Y':0,'N':1}\n",
    "Y=df.Loan_Status.map(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 0 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1\n",
      " 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0\n",
      " 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0\n",
      " 1 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0\n",
      " 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0\n",
      " 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 1 1 1\n",
      " 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0\n",
      " 0 0 1 1 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 0 1 1 1 0 1\n",
      " 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0\n",
      " 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0\n",
      " 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 1 1 0 0 1\n",
      " 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 1\n",
      " 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(Y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "rf=RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators=[int(x) for x in np.linspace(start=100,stop=1200,num=12)]\n",
    "max_features=['auto','sqrt']\n",
    "max_depth=[int(x) for x in np.linspace(start=1,stop=50,num=50)]\n",
    "min_samples_split=[int(x) for x in np.linspace(start=1,stop=50,num=50)]\n",
    "min_samples_leaf=[int(x) for x in np.linspace(start=1,stop=50,num=50)]\n",
    "oob_score=['True','False']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200], 'max_features': ['auto', 'sqrt'], 'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50], 'min_samples_split': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50], 'min_samples_leaf': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50], 'oob_score': ['True', 'False']}\n"
     ]
    }
   ],
   "source": [
    "grid={'n_estimators':n_estimators,'max_features':max_features,'max_depth':max_depth,'min_samples_split':min_samples_split,'min_samples_leaf':min_samples_leaf,'oob_score':oob_score}\n",
    "print(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions=grid,scoring='accuracy',n_iter=10,cv = 5,random_state=42,verbose=2,n_jobs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] oob_score=False, n_estimators=1200, min_samples_split=31, min_samples_leaf=11, max_features=auto, max_depth=15 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  oob_score=False, n_estimators=1200, min_samples_split=31, min_samples_leaf=11, max_features=auto, max_depth=15, total=   2.0s\n",
      "[CV] oob_score=False, n_estimators=1200, min_samples_split=31, min_samples_leaf=11, max_features=auto, max_depth=15 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  oob_score=False, n_estimators=1200, min_samples_split=31, min_samples_leaf=11, max_features=auto, max_depth=15, total=   2.0s\n",
      "[CV] oob_score=False, n_estimators=1200, min_samples_split=31, min_samples_leaf=11, max_features=auto, max_depth=15 \n",
      "[CV]  oob_score=False, n_estimators=1200, min_samples_split=31, min_samples_leaf=11, max_features=auto, max_depth=15, total=   2.0s\n",
      "[CV] oob_score=False, n_estimators=1200, min_samples_split=31, min_samples_leaf=11, max_features=auto, max_depth=15 \n",
      "[CV]  oob_score=False, n_estimators=1200, min_samples_split=31, min_samples_leaf=11, max_features=auto, max_depth=15, total=   2.0s\n",
      "[CV] oob_score=False, n_estimators=1200, min_samples_split=31, min_samples_leaf=11, max_features=auto, max_depth=15 \n",
      "[CV]  oob_score=False, n_estimators=1200, min_samples_split=31, min_samples_leaf=11, max_features=auto, max_depth=15, total=   2.0s\n",
      "[CV] oob_score=True, n_estimators=300, min_samples_split=8, min_samples_leaf=38, max_features=sqrt, max_depth=36 \n",
      "[CV]  oob_score=True, n_estimators=300, min_samples_split=8, min_samples_leaf=38, max_features=sqrt, max_depth=36, total=   0.5s\n",
      "[CV] oob_score=True, n_estimators=300, min_samples_split=8, min_samples_leaf=38, max_features=sqrt, max_depth=36 \n",
      "[CV]  oob_score=True, n_estimators=300, min_samples_split=8, min_samples_leaf=38, max_features=sqrt, max_depth=36, total=   0.5s\n",
      "[CV] oob_score=True, n_estimators=300, min_samples_split=8, min_samples_leaf=38, max_features=sqrt, max_depth=36 \n",
      "[CV]  oob_score=True, n_estimators=300, min_samples_split=8, min_samples_leaf=38, max_features=sqrt, max_depth=36, total=   0.5s\n",
      "[CV] oob_score=True, n_estimators=300, min_samples_split=8, min_samples_leaf=38, max_features=sqrt, max_depth=36 \n",
      "[CV]  oob_score=True, n_estimators=300, min_samples_split=8, min_samples_leaf=38, max_features=sqrt, max_depth=36, total=   0.5s\n",
      "[CV] oob_score=True, n_estimators=300, min_samples_split=8, min_samples_leaf=38, max_features=sqrt, max_depth=36 \n",
      "[CV]  oob_score=True, n_estimators=300, min_samples_split=8, min_samples_leaf=38, max_features=sqrt, max_depth=36, total=   0.5s\n",
      "[CV] oob_score=True, n_estimators=300, min_samples_split=21, min_samples_leaf=6, max_features=auto, max_depth=42 \n",
      "[CV]  oob_score=True, n_estimators=300, min_samples_split=21, min_samples_leaf=6, max_features=auto, max_depth=42, total=   0.5s\n",
      "[CV] oob_score=True, n_estimators=300, min_samples_split=21, min_samples_leaf=6, max_features=auto, max_depth=42 \n",
      "[CV]  oob_score=True, n_estimators=300, min_samples_split=21, min_samples_leaf=6, max_features=auto, max_depth=42, total=   0.5s\n",
      "[CV] oob_score=True, n_estimators=300, min_samples_split=21, min_samples_leaf=6, max_features=auto, max_depth=42 \n",
      "[CV]  oob_score=True, n_estimators=300, min_samples_split=21, min_samples_leaf=6, max_features=auto, max_depth=42, total=   0.5s\n",
      "[CV] oob_score=True, n_estimators=300, min_samples_split=21, min_samples_leaf=6, max_features=auto, max_depth=42 \n",
      "[CV]  oob_score=True, n_estimators=300, min_samples_split=21, min_samples_leaf=6, max_features=auto, max_depth=42, total=   0.5s\n",
      "[CV] oob_score=True, n_estimators=300, min_samples_split=21, min_samples_leaf=6, max_features=auto, max_depth=42 \n",
      "[CV]  oob_score=True, n_estimators=300, min_samples_split=21, min_samples_leaf=6, max_features=auto, max_depth=42, total=   0.5s\n",
      "[CV] oob_score=False, n_estimators=900, min_samples_split=4, min_samples_leaf=13, max_features=sqrt, max_depth=19 \n",
      "[CV]  oob_score=False, n_estimators=900, min_samples_split=4, min_samples_leaf=13, max_features=sqrt, max_depth=19, total=   1.5s\n",
      "[CV] oob_score=False, n_estimators=900, min_samples_split=4, min_samples_leaf=13, max_features=sqrt, max_depth=19 \n",
      "[CV]  oob_score=False, n_estimators=900, min_samples_split=4, min_samples_leaf=13, max_features=sqrt, max_depth=19, total=   1.5s\n",
      "[CV] oob_score=False, n_estimators=900, min_samples_split=4, min_samples_leaf=13, max_features=sqrt, max_depth=19 \n",
      "[CV]  oob_score=False, n_estimators=900, min_samples_split=4, min_samples_leaf=13, max_features=sqrt, max_depth=19, total=   1.4s\n",
      "[CV] oob_score=False, n_estimators=900, min_samples_split=4, min_samples_leaf=13, max_features=sqrt, max_depth=19 \n",
      "[CV]  oob_score=False, n_estimators=900, min_samples_split=4, min_samples_leaf=13, max_features=sqrt, max_depth=19, total=   1.5s\n",
      "[CV] oob_score=False, n_estimators=900, min_samples_split=4, min_samples_leaf=13, max_features=sqrt, max_depth=19 \n",
      "[CV]  oob_score=False, n_estimators=900, min_samples_split=4, min_samples_leaf=13, max_features=sqrt, max_depth=19, total=   1.5s\n",
      "[CV] oob_score=True, n_estimators=1200, min_samples_split=17, min_samples_leaf=9, max_features=auto, max_depth=14 \n",
      "[CV]  oob_score=True, n_estimators=1200, min_samples_split=17, min_samples_leaf=9, max_features=auto, max_depth=14, total=   1.9s\n",
      "[CV] oob_score=True, n_estimators=1200, min_samples_split=17, min_samples_leaf=9, max_features=auto, max_depth=14 \n",
      "[CV]  oob_score=True, n_estimators=1200, min_samples_split=17, min_samples_leaf=9, max_features=auto, max_depth=14, total=   1.9s\n",
      "[CV] oob_score=True, n_estimators=1200, min_samples_split=17, min_samples_leaf=9, max_features=auto, max_depth=14 \n",
      "[CV]  oob_score=True, n_estimators=1200, min_samples_split=17, min_samples_leaf=9, max_features=auto, max_depth=14, total=   1.9s\n",
      "[CV] oob_score=True, n_estimators=1200, min_samples_split=17, min_samples_leaf=9, max_features=auto, max_depth=14 \n",
      "[CV]  oob_score=True, n_estimators=1200, min_samples_split=17, min_samples_leaf=9, max_features=auto, max_depth=14, total=   1.9s\n",
      "[CV] oob_score=True, n_estimators=1200, min_samples_split=17, min_samples_leaf=9, max_features=auto, max_depth=14 \n",
      "[CV]  oob_score=True, n_estimators=1200, min_samples_split=17, min_samples_leaf=9, max_features=auto, max_depth=14, total=   2.2s\n",
      "[CV] oob_score=True, n_estimators=600, min_samples_split=37, min_samples_leaf=47, max_features=auto, max_depth=10 \n",
      "[CV]  oob_score=True, n_estimators=600, min_samples_split=37, min_samples_leaf=47, max_features=auto, max_depth=10, total=   0.9s\n",
      "[CV] oob_score=True, n_estimators=600, min_samples_split=37, min_samples_leaf=47, max_features=auto, max_depth=10 \n",
      "[CV]  oob_score=True, n_estimators=600, min_samples_split=37, min_samples_leaf=47, max_features=auto, max_depth=10, total=   0.9s\n",
      "[CV] oob_score=True, n_estimators=600, min_samples_split=37, min_samples_leaf=47, max_features=auto, max_depth=10 \n",
      "[CV]  oob_score=True, n_estimators=600, min_samples_split=37, min_samples_leaf=47, max_features=auto, max_depth=10, total=   0.9s\n",
      "[CV] oob_score=True, n_estimators=600, min_samples_split=37, min_samples_leaf=47, max_features=auto, max_depth=10 \n",
      "[CV]  oob_score=True, n_estimators=600, min_samples_split=37, min_samples_leaf=47, max_features=auto, max_depth=10, total=   0.9s\n",
      "[CV] oob_score=True, n_estimators=600, min_samples_split=37, min_samples_leaf=47, max_features=auto, max_depth=10 \n",
      "[CV]  oob_score=True, n_estimators=600, min_samples_split=37, min_samples_leaf=47, max_features=auto, max_depth=10, total=   1.2s\n",
      "[CV] oob_score=False, n_estimators=1200, min_samples_split=37, min_samples_leaf=21, max_features=sqrt, max_depth=34 \n",
      "[CV]  oob_score=False, n_estimators=1200, min_samples_split=37, min_samples_leaf=21, max_features=sqrt, max_depth=34, total=   1.9s\n",
      "[CV] oob_score=False, n_estimators=1200, min_samples_split=37, min_samples_leaf=21, max_features=sqrt, max_depth=34 \n",
      "[CV]  oob_score=False, n_estimators=1200, min_samples_split=37, min_samples_leaf=21, max_features=sqrt, max_depth=34, total=   1.9s\n",
      "[CV] oob_score=False, n_estimators=1200, min_samples_split=37, min_samples_leaf=21, max_features=sqrt, max_depth=34 \n",
      "[CV]  oob_score=False, n_estimators=1200, min_samples_split=37, min_samples_leaf=21, max_features=sqrt, max_depth=34, total=   2.3s\n",
      "[CV] oob_score=False, n_estimators=1200, min_samples_split=37, min_samples_leaf=21, max_features=sqrt, max_depth=34 \n",
      "[CV]  oob_score=False, n_estimators=1200, min_samples_split=37, min_samples_leaf=21, max_features=sqrt, max_depth=34, total=   2.1s\n",
      "[CV] oob_score=False, n_estimators=1200, min_samples_split=37, min_samples_leaf=21, max_features=sqrt, max_depth=34 \n",
      "[CV]  oob_score=False, n_estimators=1200, min_samples_split=37, min_samples_leaf=21, max_features=sqrt, max_depth=34, total=   2.0s\n",
      "[CV] oob_score=False, n_estimators=1200, min_samples_split=13, min_samples_leaf=34, max_features=auto, max_depth=11 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  oob_score=False, n_estimators=1200, min_samples_split=13, min_samples_leaf=34, max_features=auto, max_depth=11, total=   2.3s\n",
      "[CV] oob_score=False, n_estimators=1200, min_samples_split=13, min_samples_leaf=34, max_features=auto, max_depth=11 \n",
      "[CV]  oob_score=False, n_estimators=1200, min_samples_split=13, min_samples_leaf=34, max_features=auto, max_depth=11, total=   1.9s\n",
      "[CV] oob_score=False, n_estimators=1200, min_samples_split=13, min_samples_leaf=34, max_features=auto, max_depth=11 \n",
      "[CV]  oob_score=False, n_estimators=1200, min_samples_split=13, min_samples_leaf=34, max_features=auto, max_depth=11, total=   2.1s\n",
      "[CV] oob_score=False, n_estimators=1200, min_samples_split=13, min_samples_leaf=34, max_features=auto, max_depth=11 \n",
      "[CV]  oob_score=False, n_estimators=1200, min_samples_split=13, min_samples_leaf=34, max_features=auto, max_depth=11, total=   2.1s\n",
      "[CV] oob_score=False, n_estimators=1200, min_samples_split=13, min_samples_leaf=34, max_features=auto, max_depth=11 \n",
      "[CV]  oob_score=False, n_estimators=1200, min_samples_split=13, min_samples_leaf=34, max_features=auto, max_depth=11, total=   2.4s\n",
      "[CV] oob_score=False, n_estimators=1200, min_samples_split=3, min_samples_leaf=28, max_features=auto, max_depth=38 \n",
      "[CV]  oob_score=False, n_estimators=1200, min_samples_split=3, min_samples_leaf=28, max_features=auto, max_depth=38, total=   2.3s\n",
      "[CV] oob_score=False, n_estimators=1200, min_samples_split=3, min_samples_leaf=28, max_features=auto, max_depth=38 \n",
      "[CV]  oob_score=False, n_estimators=1200, min_samples_split=3, min_samples_leaf=28, max_features=auto, max_depth=38, total=   1.9s\n",
      "[CV] oob_score=False, n_estimators=1200, min_samples_split=3, min_samples_leaf=28, max_features=auto, max_depth=38 \n",
      "[CV]  oob_score=False, n_estimators=1200, min_samples_split=3, min_samples_leaf=28, max_features=auto, max_depth=38, total=   2.5s\n",
      "[CV] oob_score=False, n_estimators=1200, min_samples_split=3, min_samples_leaf=28, max_features=auto, max_depth=38 \n",
      "[CV]  oob_score=False, n_estimators=1200, min_samples_split=3, min_samples_leaf=28, max_features=auto, max_depth=38, total=   2.1s\n",
      "[CV] oob_score=False, n_estimators=1200, min_samples_split=3, min_samples_leaf=28, max_features=auto, max_depth=38 \n",
      "[CV]  oob_score=False, n_estimators=1200, min_samples_split=3, min_samples_leaf=28, max_features=auto, max_depth=38, total=   2.3s\n",
      "[CV] oob_score=True, n_estimators=600, min_samples_split=44, min_samples_leaf=32, max_features=sqrt, max_depth=18 \n",
      "[CV]  oob_score=True, n_estimators=600, min_samples_split=44, min_samples_leaf=32, max_features=sqrt, max_depth=18, total=   1.0s\n",
      "[CV] oob_score=True, n_estimators=600, min_samples_split=44, min_samples_leaf=32, max_features=sqrt, max_depth=18 \n",
      "[CV]  oob_score=True, n_estimators=600, min_samples_split=44, min_samples_leaf=32, max_features=sqrt, max_depth=18, total=   1.0s\n",
      "[CV] oob_score=True, n_estimators=600, min_samples_split=44, min_samples_leaf=32, max_features=sqrt, max_depth=18 \n",
      "[CV]  oob_score=True, n_estimators=600, min_samples_split=44, min_samples_leaf=32, max_features=sqrt, max_depth=18, total=   0.9s\n",
      "[CV] oob_score=True, n_estimators=600, min_samples_split=44, min_samples_leaf=32, max_features=sqrt, max_depth=18 \n",
      "[CV]  oob_score=True, n_estimators=600, min_samples_split=44, min_samples_leaf=32, max_features=sqrt, max_depth=18, total=   1.1s\n",
      "[CV] oob_score=True, n_estimators=600, min_samples_split=44, min_samples_leaf=32, max_features=sqrt, max_depth=18 \n",
      "[CV]  oob_score=True, n_estimators=600, min_samples_split=44, min_samples_leaf=32, max_features=sqrt, max_depth=18, total=   1.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=1,\n",
       "                   param_distributions={'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9,\n",
       "                                                      10, 11, 12, 13, 14, 15,\n",
       "                                                      16, 17, 18, 19, 20, 21,\n",
       "                                                      22, 23, 24, 25, 26, 27,\n",
       "                                                      28, 29, 30, ...],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 3, 4, 5, 6,\n",
       "                                                             7, 8, 9, 10, 11,\n",
       "                                                             12, 13, 14, 15, 16,\n",
       "                                                             17, 18, 19, 20, 21,\n",
       "                                                             22, 23, 24, 25, 26,\n",
       "                                                             27, 28, 29, 30, ...],\n",
       "                                        'min_samples_split': [1, 2, 3, 4, 5, 6,\n",
       "                                                              7, 8, 9, 10, 11,\n",
       "                                                              12, 13, 14, 15,\n",
       "                                                              16, 17, 18, 19,\n",
       "                                                              20, 21, 22, 23,\n",
       "                                                              24, 25, 26, 27,\n",
       "                                                              28, 29, 30, ...],\n",
       "                                        'n_estimators': [100, 200, 300, 400,\n",
       "                                                         500, 600, 700, 800,\n",
       "                                                         900, 1000, 1100,\n",
       "                                                         1200],\n",
       "                                        'oob_score': ['True', 'False']},\n",
       "                   random_state=42, scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.fit(X_train,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'oob_score': 'False',\n",
       " 'n_estimators': 1200,\n",
       " 'min_samples_split': 31,\n",
       " 'min_samples_leaf': 11,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 15}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.809462881514061"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn=KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors=[int(x) for x in np.linspace(start=1,stop=50,num=50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_grid={'n_neighbors':n_neighbors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_random = RandomizedSearchCV(estimator =knn , param_distributions=neighbor_grid,scoring='accuracy',n_iter=10,cv = 5,random_state=42,verbose=2,n_jobs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] n_neighbors=14 ..................................................\n",
      "[CV] ................................... n_neighbors=14, total=   0.0s\n",
      "[CV] n_neighbors=14 ..................................................\n",
      "[CV] ................................... n_neighbors=14, total=   0.0s\n",
      "[CV] n_neighbors=14 ..................................................\n",
      "[CV] ................................... n_neighbors=14, total=   0.0s\n",
      "[CV] n_neighbors=14 ..................................................\n",
      "[CV] ................................... n_neighbors=14, total=   0.0s\n",
      "[CV] n_neighbors=14 ..................................................\n",
      "[CV] ................................... n_neighbors=14, total=   0.0s\n",
      "[CV] n_neighbors=40 ..................................................\n",
      "[CV] ................................... n_neighbors=40, total=   0.0s\n",
      "[CV] n_neighbors=40 ..................................................\n",
      "[CV] ................................... n_neighbors=40, total=   0.0s\n",
      "[CV] n_neighbors=40 ..................................................\n",
      "[CV] ................................... n_neighbors=40, total=   0.0s\n",
      "[CV] n_neighbors=40 ..................................................\n",
      "[CV] ................................... n_neighbors=40, total=   0.0s\n",
      "[CV] n_neighbors=40 ..................................................\n",
      "[CV] ................................... n_neighbors=40, total=   0.0s\n",
      "[CV] n_neighbors=31 ..................................................\n",
      "[CV] ................................... n_neighbors=31, total=   0.0s\n",
      "[CV] n_neighbors=31 ..................................................\n",
      "[CV] ................................... n_neighbors=31, total=   0.0s\n",
      "[CV] n_neighbors=31 ..................................................\n",
      "[CV] ................................... n_neighbors=31, total=   0.0s\n",
      "[CV] n_neighbors=31 ..................................................\n",
      "[CV] ................................... n_neighbors=31, total=   0.0s\n",
      "[CV] n_neighbors=31 ..................................................\n",
      "[CV] ................................... n_neighbors=31, total=   0.0s\n",
      "[CV] n_neighbors=46 ..................................................\n",
      "[CV] ................................... n_neighbors=46, total=   0.0s\n",
      "[CV] n_neighbors=46 ..................................................\n",
      "[CV] ................................... n_neighbors=46, total=   0.0s\n",
      "[CV] n_neighbors=46 ..................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................... n_neighbors=46, total=   0.0s\n",
      "[CV] n_neighbors=46 ..................................................\n",
      "[CV] ................................... n_neighbors=46, total=   0.0s\n",
      "[CV] n_neighbors=46 ..................................................\n",
      "[CV] ................................... n_neighbors=46, total=   0.0s\n",
      "[CV] n_neighbors=18 ..................................................\n",
      "[CV] ................................... n_neighbors=18, total=   0.0s\n",
      "[CV] n_neighbors=18 ..................................................\n",
      "[CV] ................................... n_neighbors=18, total=   0.0s\n",
      "[CV] n_neighbors=18 ..................................................\n",
      "[CV] ................................... n_neighbors=18, total=   0.0s\n",
      "[CV] n_neighbors=18 ..................................................\n",
      "[CV] ................................... n_neighbors=18, total=   0.0s\n",
      "[CV] n_neighbors=18 ..................................................\n",
      "[CV] ................................... n_neighbors=18, total=   0.0s\n",
      "[CV] n_neighbors=49 ..................................................\n",
      "[CV] ................................... n_neighbors=49, total=   0.0s\n",
      "[CV] n_neighbors=49 ..................................................\n",
      "[CV] ................................... n_neighbors=49, total=   0.0s\n",
      "[CV] n_neighbors=49 ..................................................\n",
      "[CV] ................................... n_neighbors=49, total=   0.0s\n",
      "[CV] n_neighbors=49 ..................................................\n",
      "[CV] ................................... n_neighbors=49, total=   0.0s\n",
      "[CV] n_neighbors=49 ..................................................\n",
      "[CV] ................................... n_neighbors=49, total=   0.0s\n",
      "[CV] n_neighbors=27 ..................................................\n",
      "[CV] ................................... n_neighbors=27, total=   0.0s\n",
      "[CV] n_neighbors=27 ..................................................\n",
      "[CV] ................................... n_neighbors=27, total=   0.0s\n",
      "[CV] n_neighbors=27 ..................................................\n",
      "[CV] ................................... n_neighbors=27, total=   0.0s\n",
      "[CV] n_neighbors=27 ..................................................\n",
      "[CV] ................................... n_neighbors=27, total=   0.0s\n",
      "[CV] n_neighbors=27 ..................................................\n",
      "[CV] ................................... n_neighbors=27, total=   0.0s\n",
      "[CV] n_neighbors=26 ..................................................\n",
      "[CV] ................................... n_neighbors=26, total=   0.0s\n",
      "[CV] n_neighbors=26 ..................................................\n",
      "[CV] ................................... n_neighbors=26, total=   0.0s\n",
      "[CV] n_neighbors=26 ..................................................\n",
      "[CV] ................................... n_neighbors=26, total=   0.0s\n",
      "[CV] n_neighbors=26 ..................................................\n",
      "[CV] ................................... n_neighbors=26, total=   0.0s\n",
      "[CV] n_neighbors=26 ..................................................\n",
      "[CV] ................................... n_neighbors=26, total=   0.0s\n",
      "[CV] n_neighbors=33 ..................................................\n",
      "[CV] ................................... n_neighbors=33, total=   0.0s\n",
      "[CV] n_neighbors=33 ..................................................\n",
      "[CV] ................................... n_neighbors=33, total=   0.0s\n",
      "[CV] n_neighbors=33 ..................................................\n",
      "[CV] ................................... n_neighbors=33, total=   0.0s\n",
      "[CV] n_neighbors=33 ..................................................\n",
      "[CV] ................................... n_neighbors=33, total=   0.0s\n",
      "[CV] n_neighbors=33 ..................................................\n",
      "[CV] ................................... n_neighbors=33, total=   0.0s\n",
      "[CV] n_neighbors=20 ..................................................\n",
      "[CV] ................................... n_neighbors=20, total=   0.0s\n",
      "[CV] n_neighbors=20 ..................................................\n",
      "[CV] ................................... n_neighbors=20, total=   0.0s\n",
      "[CV] n_neighbors=20 ..................................................\n",
      "[CV] ................................... n_neighbors=20, total=   0.0s\n",
      "[CV] n_neighbors=20 ..................................................\n",
      "[CV] ................................... n_neighbors=20, total=   0.0s\n",
      "[CV] n_neighbors=20 ..................................................\n",
      "[CV] ................................... n_neighbors=20, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=KNeighborsClassifier(), n_jobs=1,\n",
       "                   param_distributions={'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8,\n",
       "                                                        9, 10, 11, 12, 13, 14,\n",
       "                                                        15, 16, 17, 18, 19, 20,\n",
       "                                                        21, 22, 23, 24, 25, 26,\n",
       "                                                        27, 28, 29, 30, ...]},\n",
       "                   random_state=42, scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbor_random.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 49}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbor_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6872984139677463"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbor_random.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.79      0.88       525\n",
      "           1       0.43      0.92      0.58        89\n",
      "\n",
      "    accuracy                           0.81       614\n",
      "   macro avg       0.71      0.86      0.73       614\n",
      "weighted avg       0.90      0.81      0.83       614\n",
      "\n",
      "[[415 110]\n",
      " [  7  82]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "preds=rf_random.predict(X)\n",
    "print(classification_report(preds,Y))\n",
    "print(confusion_matrix(preds,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('loan_classifier.pkl','wb') as f:\n",
    "    pickle.dump(rf_random,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_estimators=[int(x) for x in np.linspace(start=100,stop=1200,num=12)]\n",
    "max_features=['auto','sqrt']\n",
    "max_depth=[int(x) for x in np.linspace(start=1,stop=50,num=50)]\n",
    "min_samples_split=[int(x) for x in np.linspace(start=1,stop=50,num=50)]\n",
    "min_samples_leaf=[int(x) for x in np.linspace(start=1,stop=50,num=50)]\n",
    "criterion=['gini','entropy']\n",
    "max_leaf_nodes=[int(x) for x in np.linspace(start=1,stop=50,num=50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': ['auto', 'sqrt'], 'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50], 'min_samples_split': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50], 'min_samples_leaf': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50], 'criterion': ['gini', 'entropy'], 'max_leaf_nodes': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]}\n"
     ]
    }
   ],
   "source": [
    "dec_grid={'max_features':max_features,'max_depth':max_depth,'min_samples_split':min_samples_split,'min_samples_leaf':min_samples_leaf,'criterion':criterion,'max_leaf_nodes':max_leaf_nodes}\n",
    "print(dec_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model=DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_random = RandomizedSearchCV(estimator =model , param_distributions=dec_grid,scoring='accuracy',n_iter=10,cv = 5,random_state=42,verbose=2,n_jobs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] min_samples_split=5, min_samples_leaf=13, max_leaf_nodes=31, max_features=sqrt, max_depth=43, criterion=entropy \n",
      "[CV]  min_samples_split=5, min_samples_leaf=13, max_leaf_nodes=31, max_features=sqrt, max_depth=43, criterion=entropy, total=   0.1s\n",
      "[CV] min_samples_split=5, min_samples_leaf=13, max_leaf_nodes=31, max_features=sqrt, max_depth=43, criterion=entropy \n",
      "[CV]  min_samples_split=5, min_samples_leaf=13, max_leaf_nodes=31, max_features=sqrt, max_depth=43, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=5, min_samples_leaf=13, max_leaf_nodes=31, max_features=sqrt, max_depth=43, criterion=entropy \n",
      "[CV]  min_samples_split=5, min_samples_leaf=13, max_leaf_nodes=31, max_features=sqrt, max_depth=43, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=5, min_samples_leaf=13, max_leaf_nodes=31, max_features=sqrt, max_depth=43, criterion=entropy \n",
      "[CV]  min_samples_split=5, min_samples_leaf=13, max_leaf_nodes=31, max_features=sqrt, max_depth=43, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=5, min_samples_leaf=13, max_leaf_nodes=31, max_features=sqrt, max_depth=43, criterion=entropy \n",
      "[CV]  min_samples_split=5, min_samples_leaf=13, max_leaf_nodes=31, max_features=sqrt, max_depth=43, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=29, min_samples_leaf=40, max_leaf_nodes=38, max_features=auto, max_depth=15, criterion=entropy \n",
      "[CV]  min_samples_split=29, min_samples_leaf=40, max_leaf_nodes=38, max_features=auto, max_depth=15, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=29, min_samples_leaf=40, max_leaf_nodes=38, max_features=auto, max_depth=15, criterion=entropy \n",
      "[CV]  min_samples_split=29, min_samples_leaf=40, max_leaf_nodes=38, max_features=auto, max_depth=15, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=29, min_samples_leaf=40, max_leaf_nodes=38, max_features=auto, max_depth=15, criterion=entropy \n",
      "[CV]  min_samples_split=29, min_samples_leaf=40, max_leaf_nodes=38, max_features=auto, max_depth=15, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=29, min_samples_leaf=40, max_leaf_nodes=38, max_features=auto, max_depth=15, criterion=entropy \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  min_samples_split=29, min_samples_leaf=40, max_leaf_nodes=38, max_features=auto, max_depth=15, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=29, min_samples_leaf=40, max_leaf_nodes=38, max_features=auto, max_depth=15, criterion=entropy \n",
      "[CV]  min_samples_split=29, min_samples_leaf=40, max_leaf_nodes=38, max_features=auto, max_depth=15, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=1, min_samples_leaf=8, max_leaf_nodes=32, max_features=auto, max_depth=44, criterion=entropy \n",
      "[CV]  min_samples_split=1, min_samples_leaf=8, max_leaf_nodes=32, max_features=auto, max_depth=44, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=1, min_samples_leaf=8, max_leaf_nodes=32, max_features=auto, max_depth=44, criterion=entropy \n",
      "[CV]  min_samples_split=1, min_samples_leaf=8, max_leaf_nodes=32, max_features=auto, max_depth=44, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=1, min_samples_leaf=8, max_leaf_nodes=32, max_features=auto, max_depth=44, criterion=entropy \n",
      "[CV]  min_samples_split=1, min_samples_leaf=8, max_leaf_nodes=32, max_features=auto, max_depth=44, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=1, min_samples_leaf=8, max_leaf_nodes=32, max_features=auto, max_depth=44, criterion=entropy \n",
      "[CV]  min_samples_split=1, min_samples_leaf=8, max_leaf_nodes=32, max_features=auto, max_depth=44, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=1, min_samples_leaf=8, max_leaf_nodes=32, max_features=auto, max_depth=44, criterion=entropy \n",
      "[CV]  min_samples_split=1, min_samples_leaf=8, max_leaf_nodes=32, max_features=auto, max_depth=44, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=39, min_samples_leaf=36, max_leaf_nodes=33, max_features=auto, max_depth=35, criterion=entropy \n",
      "[CV]  min_samples_split=39, min_samples_leaf=36, max_leaf_nodes=33, max_features=auto, max_depth=35, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=39, min_samples_leaf=36, max_leaf_nodes=33, max_features=auto, max_depth=35, criterion=entropy \n",
      "[CV]  min_samples_split=39, min_samples_leaf=36, max_leaf_nodes=33, max_features=auto, max_depth=35, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=39, min_samples_leaf=36, max_leaf_nodes=33, max_features=auto, max_depth=35, criterion=entropy \n",
      "[CV]  min_samples_split=39, min_samples_leaf=36, max_leaf_nodes=33, max_features=auto, max_depth=35, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=39, min_samples_leaf=36, max_leaf_nodes=33, max_features=auto, max_depth=35, criterion=entropy \n",
      "[CV]  min_samples_split=39, min_samples_leaf=36, max_leaf_nodes=33, max_features=auto, max_depth=35, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=39, min_samples_leaf=36, max_leaf_nodes=33, max_features=auto, max_depth=35, criterion=entropy \n",
      "[CV]  min_samples_split=39, min_samples_leaf=36, max_leaf_nodes=33, max_features=auto, max_depth=35, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=43, min_samples_leaf=2, max_leaf_nodes=27, max_features=auto, max_depth=4, criterion=entropy \n",
      "[CV]  min_samples_split=43, min_samples_leaf=2, max_leaf_nodes=27, max_features=auto, max_depth=4, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=43, min_samples_leaf=2, max_leaf_nodes=27, max_features=auto, max_depth=4, criterion=entropy \n",
      "[CV]  min_samples_split=43, min_samples_leaf=2, max_leaf_nodes=27, max_features=auto, max_depth=4, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=43, min_samples_leaf=2, max_leaf_nodes=27, max_features=auto, max_depth=4, criterion=entropy \n",
      "[CV]  min_samples_split=43, min_samples_leaf=2, max_leaf_nodes=27, max_features=auto, max_depth=4, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=43, min_samples_leaf=2, max_leaf_nodes=27, max_features=auto, max_depth=4, criterion=entropy \n",
      "[CV]  min_samples_split=43, min_samples_leaf=2, max_leaf_nodes=27, max_features=auto, max_depth=4, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=43, min_samples_leaf=2, max_leaf_nodes=27, max_features=auto, max_depth=4, criterion=entropy \n",
      "[CV]  min_samples_split=43, min_samples_leaf=2, max_leaf_nodes=27, max_features=auto, max_depth=4, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=40, min_samples_leaf=40, max_leaf_nodes=44, max_features=sqrt, max_depth=9, criterion=gini \n",
      "[CV]  min_samples_split=40, min_samples_leaf=40, max_leaf_nodes=44, max_features=sqrt, max_depth=9, criterion=gini, total=   0.0s\n",
      "[CV] min_samples_split=40, min_samples_leaf=40, max_leaf_nodes=44, max_features=sqrt, max_depth=9, criterion=gini \n",
      "[CV]  min_samples_split=40, min_samples_leaf=40, max_leaf_nodes=44, max_features=sqrt, max_depth=9, criterion=gini, total=   0.0s\n",
      "[CV] min_samples_split=40, min_samples_leaf=40, max_leaf_nodes=44, max_features=sqrt, max_depth=9, criterion=gini \n",
      "[CV]  min_samples_split=40, min_samples_leaf=40, max_leaf_nodes=44, max_features=sqrt, max_depth=9, criterion=gini, total=   0.0s\n",
      "[CV] min_samples_split=40, min_samples_leaf=40, max_leaf_nodes=44, max_features=sqrt, max_depth=9, criterion=gini \n",
      "[CV]  min_samples_split=40, min_samples_leaf=40, max_leaf_nodes=44, max_features=sqrt, max_depth=9, criterion=gini, total=   0.0s\n",
      "[CV] min_samples_split=40, min_samples_leaf=40, max_leaf_nodes=44, max_features=sqrt, max_depth=9, criterion=gini \n",
      "[CV]  min_samples_split=40, min_samples_leaf=40, max_leaf_nodes=44, max_features=sqrt, max_depth=9, criterion=gini, total=   0.0s\n",
      "[CV] min_samples_split=37, min_samples_leaf=24, max_leaf_nodes=35, max_features=auto, max_depth=9, criterion=entropy \n",
      "[CV]  min_samples_split=37, min_samples_leaf=24, max_leaf_nodes=35, max_features=auto, max_depth=9, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=37, min_samples_leaf=24, max_leaf_nodes=35, max_features=auto, max_depth=9, criterion=entropy \n",
      "[CV]  min_samples_split=37, min_samples_leaf=24, max_leaf_nodes=35, max_features=auto, max_depth=9, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=37, min_samples_leaf=24, max_leaf_nodes=35, max_features=auto, max_depth=9, criterion=entropy \n",
      "[CV]  min_samples_split=37, min_samples_leaf=24, max_leaf_nodes=35, max_features=auto, max_depth=9, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=37, min_samples_leaf=24, max_leaf_nodes=35, max_features=auto, max_depth=9, criterion=entropy \n",
      "[CV]  min_samples_split=37, min_samples_leaf=24, max_leaf_nodes=35, max_features=auto, max_depth=9, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=37, min_samples_leaf=24, max_leaf_nodes=35, max_features=auto, max_depth=9, criterion=entropy \n",
      "[CV]  min_samples_split=37, min_samples_leaf=24, max_leaf_nodes=35, max_features=auto, max_depth=9, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=29, min_samples_leaf=29, max_leaf_nodes=43, max_features=sqrt, max_depth=46, criterion=entropy \n",
      "[CV]  min_samples_split=29, min_samples_leaf=29, max_leaf_nodes=43, max_features=sqrt, max_depth=46, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=29, min_samples_leaf=29, max_leaf_nodes=43, max_features=sqrt, max_depth=46, criterion=entropy \n",
      "[CV]  min_samples_split=29, min_samples_leaf=29, max_leaf_nodes=43, max_features=sqrt, max_depth=46, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=29, min_samples_leaf=29, max_leaf_nodes=43, max_features=sqrt, max_depth=46, criterion=entropy \n",
      "[CV]  min_samples_split=29, min_samples_leaf=29, max_leaf_nodes=43, max_features=sqrt, max_depth=46, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=29, min_samples_leaf=29, max_leaf_nodes=43, max_features=sqrt, max_depth=46, criterion=entropy \n",
      "[CV]  min_samples_split=29, min_samples_leaf=29, max_leaf_nodes=43, max_features=sqrt, max_depth=46, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=29, min_samples_leaf=29, max_leaf_nodes=43, max_features=sqrt, max_depth=46, criterion=entropy \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 894, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 231, in fit\n",
      "    % self.min_samples_split)\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  FitFailedWarning)\n",
      "c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 894, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 231, in fit\n",
      "    % self.min_samples_split)\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  FitFailedWarning)\n",
      "c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 894, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 231, in fit\n",
      "    % self.min_samples_split)\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  FitFailedWarning)\n",
      "c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 894, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 231, in fit\n",
      "    % self.min_samples_split)\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  FitFailedWarning)\n",
      "c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 894, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 231, in fit\n",
      "    % self.min_samples_split)\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  min_samples_split=29, min_samples_leaf=29, max_leaf_nodes=43, max_features=sqrt, max_depth=46, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=20, min_samples_leaf=21, max_leaf_nodes=2, max_features=sqrt, max_depth=39, criterion=gini \n",
      "[CV]  min_samples_split=20, min_samples_leaf=21, max_leaf_nodes=2, max_features=sqrt, max_depth=39, criterion=gini, total=   0.0s\n",
      "[CV] min_samples_split=20, min_samples_leaf=21, max_leaf_nodes=2, max_features=sqrt, max_depth=39, criterion=gini \n",
      "[CV]  min_samples_split=20, min_samples_leaf=21, max_leaf_nodes=2, max_features=sqrt, max_depth=39, criterion=gini, total=   0.0s\n",
      "[CV] min_samples_split=20, min_samples_leaf=21, max_leaf_nodes=2, max_features=sqrt, max_depth=39, criterion=gini \n",
      "[CV]  min_samples_split=20, min_samples_leaf=21, max_leaf_nodes=2, max_features=sqrt, max_depth=39, criterion=gini, total=   0.0s\n",
      "[CV] min_samples_split=20, min_samples_leaf=21, max_leaf_nodes=2, max_features=sqrt, max_depth=39, criterion=gini \n",
      "[CV]  min_samples_split=20, min_samples_leaf=21, max_leaf_nodes=2, max_features=sqrt, max_depth=39, criterion=gini, total=   0.0s\n",
      "[CV] min_samples_split=20, min_samples_leaf=21, max_leaf_nodes=2, max_features=sqrt, max_depth=39, criterion=gini \n",
      "[CV]  min_samples_split=20, min_samples_leaf=21, max_leaf_nodes=2, max_features=sqrt, max_depth=39, criterion=gini, total=   0.0s\n",
      "[CV] min_samples_split=38, min_samples_leaf=44, max_leaf_nodes=50, max_features=sqrt, max_depth=35, criterion=entropy \n",
      "[CV]  min_samples_split=38, min_samples_leaf=44, max_leaf_nodes=50, max_features=sqrt, max_depth=35, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=38, min_samples_leaf=44, max_leaf_nodes=50, max_features=sqrt, max_depth=35, criterion=entropy \n",
      "[CV]  min_samples_split=38, min_samples_leaf=44, max_leaf_nodes=50, max_features=sqrt, max_depth=35, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=38, min_samples_leaf=44, max_leaf_nodes=50, max_features=sqrt, max_depth=35, criterion=entropy \n",
      "[CV]  min_samples_split=38, min_samples_leaf=44, max_leaf_nodes=50, max_features=sqrt, max_depth=35, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=38, min_samples_leaf=44, max_leaf_nodes=50, max_features=sqrt, max_depth=35, criterion=entropy \n",
      "[CV]  min_samples_split=38, min_samples_leaf=44, max_leaf_nodes=50, max_features=sqrt, max_depth=35, criterion=entropy, total=   0.0s\n",
      "[CV] min_samples_split=38, min_samples_leaf=44, max_leaf_nodes=50, max_features=sqrt, max_depth=35, criterion=entropy \n",
      "[CV]  min_samples_split=38, min_samples_leaf=44, max_leaf_nodes=50, max_features=sqrt, max_depth=35, criterion=entropy, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=DecisionTreeClassifier(), n_jobs=1,\n",
       "                   param_distributions={'criterion': ['gini', 'entropy'],\n",
       "                                        'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9,\n",
       "                                                      10, 11, 12, 13, 14, 15,\n",
       "                                                      16, 17, 18, 19, 20, 21,\n",
       "                                                      22, 23, 24, 25, 26, 27,\n",
       "                                                      28, 29, 30, ...],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'max_leaf_nodes': [1, 2, 3, 4, 5, 6, 7,\n",
       "                                                           8, 9, 10, 11, 12, 13,\n",
       "                                                           14, 15, 16, 17, 18,\n",
       "                                                           19, 20, 21, 22, 23,\n",
       "                                                           24, 25, 26, 27, 28,\n",
       "                                                           29, 30, ...],\n",
       "                                        'min_samples_leaf': [1, 2, 3, 4, 5, 6,\n",
       "                                                             7, 8, 9, 10, 11,\n",
       "                                                             12, 13, 14, 15, 16,\n",
       "                                                             17, 18, 19, 20, 21,\n",
       "                                                             22, 23, 24, 25, 26,\n",
       "                                                             27, 28, 29, 30, ...],\n",
       "                                        'min_samples_split': [1, 2, 3, 4, 5, 6,\n",
       "                                                              7, 8, 9, 10, 11,\n",
       "                                                              12, 13, 14, 15,\n",
       "                                                              16, 17, 18, 19,\n",
       "                                                              20, 21, 22, 23,\n",
       "                                                              24, 25, 26, 27,\n",
       "                                                              28, 29, 30, ...]},\n",
       "                   random_state=42, scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_random.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7899506863921097"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_random.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_samples_split': 40,\n",
       " 'min_samples_leaf': 40,\n",
       " 'max_leaf_nodes': 44,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 9,\n",
       " 'criterion': 'gini'}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_model=LogisticRegression()\n",
    "#lr_model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l2\",\"elasticnet\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "logreg_cv=GridSearchCV(lr_model,grid,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\users\\user\\anaconda3\\envs\\tfp3.7\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=LogisticRegression(),\n",
       "             param_grid={'C': array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n",
       "                         'penalty': ['l2', 'elasticnet']})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_cv.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.69      0.81       612\n",
      "           1       0.01      0.50      0.01         2\n",
      "\n",
      "    accuracy                           0.69       614\n",
      "   macro avg       0.50      0.59      0.41       614\n",
      "weighted avg       0.99      0.69      0.81       614\n",
      "\n",
      "[[421 191]\n",
      " [  1   1]]\n"
     ]
    }
   ],
   "source": [
    "ypreds=logreg_cv.predict(X)\n",
    "print(classification_report(ypreds,Y))\n",
    "print(confusion_matrix(ypreds,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('loan_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['Married','Education','Self_Employed','ApplicantIncome','CoapplicantIncome','LoanAmount','Loan_Amount_Term','Credit_History','Property_Area']\n",
    "df['LoanAmount']=df['LoanAmount']*1000*74.81\n",
    "df['ApplicantIncome']=df['ApplicantIncome']*74.81\n",
    "df['CoapplicantIncome']=df['CoapplicantIncome']*74.81\n",
    "X=df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.get_dummies(X,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['Married_Yes']=X['Married_Yes'].fillna(value=0)\n",
    "X['Self_Employed_Yes']=X['Self_Employed_Yes'].fillna(value=0)\n",
    "X['LoanAmount']=X['LoanAmount'].fillna(value=df['LoanAmount'].value_counts().idxmax())\n",
    "X['Loan_Amount_Term']=X['Loan_Amount_Term'].fillna(value=X['Loan_Amount_Term'].value_counts().idxmax())\n",
    "X['Credit_History']=X['Credit_History'].fillna(value=X['Credit_History'].value_counts().idxmax())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc=MinMaxScaler(feature_range = (0,1)).fit(X.values)\n",
    "X_test=sc.transform(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "preds=rf_random.predict(X_test)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Loan_ID  Loan_Status\n",
      "0    LP001015            0\n",
      "1    LP001022            0\n",
      "2    LP001031            0\n",
      "3    LP001035            0\n",
      "4    LP001051            0\n",
      "..        ...          ...\n",
      "362  LP002971            0\n",
      "363  LP002975            0\n",
      "364  LP002980            0\n",
      "365  LP002986            0\n",
      "366  LP002989            0\n",
      "\n",
      "[367 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "y='Y'\n",
    "n='N'\n",
    "mapping={0:y,1:n}\n",
    "data=pd.DataFrame({'Loan_ID':df.Loan_ID,'Loan_Status':preds})\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      Y\n",
      "1      Y\n",
      "2      Y\n",
      "3      Y\n",
      "4      Y\n",
      "      ..\n",
      "362    Y\n",
      "363    Y\n",
      "364    Y\n",
      "365    Y\n",
      "366    Y\n",
      "Name: Loan_Status, Length: 367, dtype: object\n"
     ]
    }
   ],
   "source": [
    "Loan_Status=data.Loan_Status.map(mapping)\n",
    "print(Loan_Status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub=pd.DataFrame({'Loan_ID':df.Loan_ID,'Loan_Status':Loan_Status})\n",
    "sub.to_csv('loan_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
